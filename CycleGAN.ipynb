{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NSLwag6YUbiS",
    "outputId": "dee72b69-e751-403d-b4a1-19e01cc8754e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "_YqVCWtXUjwv",
    "outputId": "3904c4c7-5c49-4aff-a0d3-7153d3c052f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Jbs01lbakT9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JobsH4RaakXK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qMaEJELTUkCI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ll0BD6pEUbiY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUIC2T_XUbid"
   },
   "source": [
    "##############################################################################################################\n",
    "################################################# HELPER FUNCTIONS ###########################################\n",
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7DtbgIlUbie"
   },
   "outputs": [],
   "source": [
    "\n",
    "class helper_functions:\n",
    "    \n",
    "    ########################################\n",
    "    \n",
    "    def get_weights(self, shape, name):\n",
    "        with tf.variable_scope('weights', reuse = tf.AUTO_REUSE):\n",
    "            wt_init = tf.random_normal_initializer()\n",
    "            return tf.get_variable(name = name, shape = shape, initializer = wt_init)\n",
    "        \n",
    "        \n",
    "        \n",
    "    ########################################\n",
    "    \n",
    "    def get_bias(self, shape, name):\n",
    "        with tf.variable_scope('biases', reuse = tf.AUTO_REUSE):\n",
    "            init = tf.constant_initializer(0)\n",
    "            return tf.get_variable(name = name, shape = shape, initializer = init)\n",
    "        \n",
    "        \n",
    "        \n",
    "    ########################################\n",
    "    \n",
    "    def apply_activations(self, data, activation):\n",
    "        assert activation in ['relu', 'leaky_relu', 'tanh', 'sigmoid', None]\n",
    "        \n",
    "        if activation   == 'relu':\n",
    "            return tf.nn.relu(data)\n",
    "        elif activation == 'leaky_relu':\n",
    "            return tf.contrib.keras.layers.LeakyReLU(0.2)(data)\n",
    "        elif activation == 'tanh':\n",
    "            return tf.tanh(data)\n",
    "        elif activation == 'sigmoid':\n",
    "            return tf.sigmoid(data)\n",
    "        else:\n",
    "            return data\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################################\n",
    "    \n",
    "    def batch_normalization(self, data, is_training, norm_mode = None):\n",
    "    \n",
    "        if norm_mode == 'instance':\n",
    "            with tf.variable_scope('instance_norm', reuse = tf.AUTO_REUSE):\n",
    "                eps         = 1e-5\n",
    "                mean, sigma = tf.nn.moments(data, [1, 2], keep_dims=True)\n",
    "                normalized  = (data - mean) / (tf.sqrt(sigma) + eps)\n",
    "                out         = normalized\n",
    "\n",
    "        elif norm_mode     == 'batch':\n",
    "            with tf.variable_scope('batch_norm', reuse= tf.AUTO_REUSE):\n",
    "                out         = tf.contrib.layers.batch_norm(data, decay = 0.99, center = True, scale = True, \n",
    "                                                   is_training=is_training, updates_collections = None)\n",
    "        else:\n",
    "            out = data\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "    \n",
    "    ########################################\n",
    "    \n",
    "    def conv_2d_transpose(self, data, weights, strides, output_shape):\n",
    "#         print('data ' + str(data.get_shape().as_list()))\n",
    "#         print('weights ' + str(weights.get_shape().as_list()))\n",
    "#         print('strides ' + str(strides))\n",
    "#         print('op shape ' + str(output_shape))\n",
    "        return tf.nn.conv2d_transpose(data, weights, tf.convert_to_tensor(output_shape), strides, 'SAME')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################################\n",
    "    \n",
    "    def conv_block(self, data, weights, name, strides, is_training, norm_mode, activation, bias = None):\n",
    "\n",
    "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "            if bias is not None:\n",
    "                res   = tf.nn.conv2d(data, filter = weights, strides = strides, padding = 'SAME')\n",
    "                res   = tf.nn.bias_add(res, bias)\n",
    "            else:\n",
    "                res   = conv_2d_transpose()\n",
    "                \n",
    "            conv_bn   = self.batch_normalization(res, is_training, norm_mode)\n",
    "            conv_actv = self.apply_activations(conv_bn, activation)\n",
    "            return conv_actv\n",
    "        \n",
    "    \n",
    "    \n",
    "    ########################################\n",
    "    \n",
    "    def residual_block(self, data, weights, bias, name, strides, is_training, norm_mode, activation):\n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            with tf.variable_scope('res_1', reuse = tf.AUTO_REUSE):\n",
    "                res_1 = self.conv_block(data, weights[0], 'conv_1', strides, is_training, norm_mode, activation, bias[0])\n",
    "            \n",
    "            with tf.variable_scope('res_2', reuse = tf.AUTO_REUSE):\n",
    "                with tf.variable_scope('conv_2', reuse = tf.AUTO_REUSE):\n",
    "                    conv_res_2      = tf.nn.conv2d(res_1, weights[1], strides, padding = 'SAME')\n",
    "                    conv_res_2_bias = tf.nn.bias_add(conv_res_2, bias[1])\n",
    "                    conv_res_2_bn   = self.batch_normalization(conv_res_2_bias, is_training, norm_mode)\n",
    "            \n",
    "            return tf.nn.relu(conv_res_2_bn + data)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    ########################################\n",
    "    \n",
    "    def deconv_block(self, data, weights, name, strides, is_training, norm_mode, activation, output_shape):\n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            deconv_res = self.conv_2d_transpose(data, weights, strides, output_shape)\n",
    "            deconv_norm = self.batch_normalization(deconv_res, is_training, norm_mode)\n",
    "            deconv_actv = self.apply_activations(deconv_norm, activation)\n",
    "            return deconv_actv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_l4VT0UUbih"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nncESxiVUbij"
   },
   "source": [
    "##############################################################################################################\n",
    "################################################# GENERATOR  #################################################\n",
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BjUUZ7ZUbik"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Generator:\n",
    "    \n",
    "    ########################################\n",
    "    \n",
    "    def __init__(self, name, image_shape, is_training, norm_mode, activation, batch_size, helper_functions):\n",
    "        self.image_shape   = image_shape\n",
    "        self.batch_size    = batch_size\n",
    "        self.helper        = helper_functions\n",
    "        self.no_res_blocks = 6 if self.image_shape[0] <= 128 else 9\n",
    "        self.is_training   = is_training\n",
    "        self.norm_mode     = norm_mode\n",
    "        self.activation    = activation\n",
    "        self.g_filters     = [32, 64, 128] if self.image_shape[0] <= 128 else [32, 128, 256]\n",
    "        self.t_filter_size = 3\n",
    "        self.t_num_filters = 128 if self.image_shape[0] <= 128 else 256\n",
    "        self.t_resd_stride = [1, 1, 1, 1]\n",
    "        self.g_name        = name\n",
    "        \n",
    "        \n",
    "    \n",
    "    ###########################################################################################################\n",
    "    \n",
    "                                        ### ENCODING BLOCK ###\n",
    "    \n",
    "    ###########################################################################################################\n",
    "    \n",
    "    def Encoding_phase(self, data):\n",
    "\n",
    "        self.g_en_wts_1  = self.helper.get_weights([7, 7, data.get_shape()[3].value, self.g_filters[0]], 'g_en_wts_1')\n",
    "        self.g_en_bias_1 = self.helper.get_bias([self.g_filters[0]], 'g_en_bias_1')\n",
    "        enc_conv_1       = self.helper.conv_block(data, self.g_en_wts_1, 'g_conv_1', [1, 1, 1, 1,],\n",
    "                                            self.is_training, self.norm_mode, self.activation, self.g_en_bias_1)\n",
    "    \n",
    "        \n",
    "        self.g_en_wts_2  = self.helper.get_weights([3, 3, self.g_filters[0], self.g_filters[1]], 'g_en_wts_2')\n",
    "        self.g_en_bias_2 = self.helper.get_bias([self.g_filters[1]], 'g_en_bias_2')\n",
    "        enc_conv_2       = self.helper.conv_block(enc_conv_1, self.g_en_wts_2, 'g_conv_2', [1, 2, 2, 1],\n",
    "                                            self.is_training, self.norm_mode, self.activation, self.g_en_bias_2)\n",
    "        \n",
    "        self.g_en_wts_3  = self.helper.get_weights([3, 3, self.g_filters[1], self.g_filters[2]], 'g_en_wts_3')\n",
    "        self.g_en_bias_3 = self.helper.get_bias([self.g_filters[2]], 'g_en_bias_3')\n",
    "        enc_conv_3       = self.helper.conv_block(enc_conv_2, self.g_en_wts_3, 'g_conv_3', [1, 2, 2, 1],\n",
    "                                            self.is_training, self.norm_mode, self.activation, self.g_en_bias_3)\n",
    "        \n",
    "        return enc_conv_3\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########################################################################################################\n",
    "    \n",
    "                                        ### TRANSFORMATION BLOCK ###\n",
    "    \n",
    "    ###########################################################################################################\n",
    "    \n",
    "    def add_resd_weights_to_class_obj(self, data):\n",
    "        \n",
    "        for res_block_no in range(0, self.no_res_blocks):\n",
    "            for sub_block_no in range(0, 2):\n",
    "                channels = self.t_num_filters\n",
    "                if res_block_no == 0 and sub_block_no == 0:\n",
    "                    channels = data.get_shape().as_list()[3]\n",
    "                \n",
    "                wts_name     = 'g_resd_wts_' + str(res_block_no) + '_' + str(sub_block_no)\n",
    "                bias_name    = 'g_resd_bias_' + str(res_block_no) + '_' + str(sub_block_no)\n",
    "                filter_shape = [self.t_filter_size, self.t_filter_size, channels, self.t_num_filters]\n",
    "            \n",
    "                weights = self.helper.get_weights(filter_shape, wts_name)\n",
    "                bias    = self.helper.get_bias([self.t_num_filters], bias_name)\n",
    "                setattr(self, wts_name, weights)\n",
    "                setattr(self, bias_name, bias)\n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################\n",
    "    \n",
    "    def Transformation_phase(self, data):\n",
    "        self.add_resd_weights_to_class_obj(data)\n",
    "        resd_weights_dict = vars(self)\n",
    "        \n",
    "        t_output = data\n",
    "        \n",
    "        for res_block_no in range(0, self.no_res_blocks):\n",
    "            weights_li = []\n",
    "            bias_li    = []\n",
    "            wts_name   = 'g_resd_wts_' + str(res_block_no) +'_'\n",
    "            bias_name  = 'g_resd_bias_' + str(res_block_no) + '_'\n",
    "            \n",
    "            weights_li.append(resd_weights_dict[wts_name + str(0)])\n",
    "            weights_li.append(resd_weights_dict[wts_name + str(1)])\n",
    "            \n",
    "            bias_li.append(resd_weights_dict[bias_name + str(0)])\n",
    "            bias_li.append(resd_weights_dict[bias_name + str(1)])\n",
    "            t_output = self.helper.residual_block(t_output, weights_li, bias_li, ('g_t_resd_' + str(res_block_no)),\n",
    "                                            self.t_resd_stride, self.is_training, self.norm_mode, self.activation)\n",
    "        \n",
    "        \n",
    "        return t_output\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########################################################################################################\n",
    "    \n",
    "                                        ### DECODING BLOCK ###\n",
    "    \n",
    "    ###########################################################################################################\n",
    "    \n",
    "    def get_filters_and_op_shape(self, data, filter_size, num_filters, stride):\n",
    "        batch_size, h, w, c = data.get_shape().as_list()\n",
    "        filter_shape = [filter_size, filter_size, num_filters, c]\n",
    "        op_shape   = [1, h * stride, w * stride, num_filters]\n",
    "        return filter_shape, op_shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##########################################\n",
    "    \n",
    "    def Decoding_phase(self, data):\n",
    "\n",
    "        filter_shape, op_shape = self.get_filters_and_op_shape(data, 3, self.g_filters[1], 2)\n",
    "        \n",
    "#         print('opshape - Gen - deco ' +  str(op_shape))\n",
    "#         print('filter_shape ')\n",
    "        self.g_dec_wts_1 = self.helper.get_weights(filter_shape, 'g_dec_wts_1')\n",
    "        dec_deconv_1     = self.helper.deconv_block(data, self.g_dec_wts_1, 'g_deconv_1', [1, 2, 2, 1], \n",
    "                                                self.is_training, self.norm_mode, self.activation, op_shape)\n",
    "        \n",
    "        filter_shape, op_shape = self.get_filters_and_op_shape(dec_deconv_1, 3, self.g_filters[0], 2)\n",
    "        self.g_dec_wts_2 = self.helper.get_weights(filter_shape, 'g_dec_wts_2')\n",
    "        dec_deconv_2     = self.helper.deconv_block(dec_deconv_1, self.g_dec_wts_2, 'g_deconv_2', [1, 2, 2, 1], \n",
    "                                                self.is_training, self.norm_mode, self.activation, op_shape)\n",
    "        \n",
    "        \n",
    "        filter_shape, op_shape = self.get_filters_and_op_shape(dec_deconv_2, 7, 3, 1)\n",
    "        self.g_dec_wts_3 = self.helper.get_weights(filter_shape, 'g_dec_wts_3')\n",
    "        dec_deconv_3     = self.helper.deconv_block(dec_deconv_2, self.g_dec_wts_3, 'g_deconv_3', [1, 1, 1, 1],\n",
    "                                                self.is_training, None, 'tanh', op_shape)\n",
    "        \n",
    "        return dec_deconv_3\n",
    "        \n",
    "        \n",
    "    \n",
    "    ###########################################################################################################\n",
    "    \n",
    "                                        ###  FORWARD PROPAGATION ###\n",
    "    \n",
    "    ###########################################################################################################\n",
    "    \n",
    "    def g_feed_forward(self, data):\n",
    "        with tf.variable_scope(self.g_name, reuse = tf.AUTO_REUSE):\n",
    "            encoding_res       = self.Encoding_phase(data)\n",
    "            transformation_res = self.Transformation_phase(encoding_res)\n",
    "            decoding_res       = self.Decoding_phase(transformation_res)\n",
    "            \n",
    "            self.g_variables     = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.g_name)\n",
    "            return decoding_res\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p5dkGTgnUbim"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "slBndC33Ubir"
   },
   "source": [
    "##############################################################################################################\n",
    "#################################################  DISCRIMINATOR ##########################################\n",
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anM3X1m_Ubis"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Discriminator:\n",
    "    def __init__(self, name, is_training, norm, activation, helper_functions):\n",
    "        self.d_name          = name\n",
    "        self.is_training   = is_training\n",
    "        self.norm_mode     = norm\n",
    "        self.activation    = activation\n",
    "        self.d_filter_size = 4\n",
    "        self.d_num_filters = [64, 128, 256, 512, 1]\n",
    "        self.conv_stride   = [1, 2, 2, 1]\n",
    "        self.helper = helper_functions\n",
    "        \n",
    "        \n",
    "    def d_feed_forward(self, data):\n",
    "        with tf.variable_scope(self.d_name, reuse = tf.AUTO_REUSE):\n",
    "            self.d_wts_1  = self.helper.get_weights([4, 4, data.get_shape()[3].value, self.d_num_filters[0]], 'd_wts_1')\n",
    "            self.d_bias_1 = self.helper.get_bias([self.d_num_filters[0]], 'd_bias_1')\n",
    "            disc_conv_1   = self.helper.conv_block(data, self.d_wts_1, 'd_conv_1', self.conv_stride,\n",
    "                                                self.is_training, self.norm_mode, self.activation, self.d_bias_1)\n",
    "\n",
    "\n",
    "            self.d_wts_2  = self.helper.get_weights([4, 4, self.d_num_filters[0], self.d_num_filters[1]], 'd_wts_2')\n",
    "            self.d_bias_2 = self.helper.get_bias([self.d_num_filters[1]], 'd_bias_2')\n",
    "            disc_conv_2       = self.helper.conv_block(disc_conv_1, self.d_wts_2, 'd_conv_2', self.conv_stride,\n",
    "                                                self.is_training, self.norm_mode, self.activation, self.d_bias_2)\n",
    "\n",
    "            self.d_wts_3  = self.helper.get_weights([4, 4, self.d_num_filters[1], self.d_num_filters[2]], 'd_wts_3')\n",
    "            self.d_bias_3 = self.helper.get_bias([self.d_num_filters[2]], 'd_bias_3')\n",
    "            disc_conv_3       = self.helper.conv_block(disc_conv_2, self.d_wts_3, 'd_conv_3', self.conv_stride,\n",
    "                                                self.is_training, self.norm_mode, self.activation, self.d_bias_3)\n",
    "\n",
    "\n",
    "            self.d_wts_4 = self.helper.get_weights([4, 4, self.d_num_filters[2], self.d_num_filters[3]], 'd_wts_4')\n",
    "            self.d_bias_4 = self.helper.get_bias([self.d_num_filters[3]], 'd_bias_4')\n",
    "            disc_conv_4  = self.helper.conv_block(disc_conv_3, self.d_wts_4, 'd_conv_4', self.conv_stride,\n",
    "                                                self.is_training, self.norm_mode, self.activation, self.d_bias_4)\n",
    "\n",
    "            self.d_wts_5 = self.helper.get_weights([4, 4, self.d_num_filters[3], self.d_num_filters[4]], 'd_wts_5')\n",
    "            self.d_bias_5 = self.helper.get_bias([self.d_num_filters[4]], 'd_bias_5')\n",
    "            disc_conv_5  = self.helper.conv_block(disc_conv_4, self.d_wts_5, 'd_conv_5', [1, 1, 1, 1], \n",
    "                                                self.is_training, None, None, self.d_bias_5)\n",
    "\n",
    "            conv_5_res = tf.reduce_mean(disc_conv_5, axis = [1, 2, 3])\n",
    "            self.d_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.d_name)\n",
    "            \n",
    "            return conv_5_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8IRkjpebUbiv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F9cNW3C7Ubix"
   },
   "source": [
    "##############################################################################################################\n",
    "################################################# TRAINING CYCLEGAN ##########################################\n",
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gt-U5_fVUbix"
   },
   "outputs": [],
   "source": [
    "\n",
    "class cycleGAN:\n",
    "    def __init__(self, image_shape, batch_size, learning_rate, train_data_path, test_data_path, num_epochs, \n",
    "                                             to_restore, helper_functions, output_dir):\n",
    "        self.image_shape = image_shape\n",
    "        self.batch_size  = batch_size\n",
    "        self.is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
    "        self.lr = learning_rate\n",
    "        self.X = tf.placeholder(tf.float32, [None, self.image_shape[0], self.image_shape[1], self.image_shape[2]])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, self.image_shape[0], self.image_shape[1], self.image_shape[2]])\n",
    "        self.train_data_path = train_data_path\n",
    "        self.test_data_path = test_data_path\n",
    "        self.helper = helper_functions\n",
    "        self.num_of_epochs = num_epochs\n",
    "        self.to_restore = to_restore\n",
    "        self.output_dir = output_dir\n",
    "        self.check_point_dir = self.output_dir + '/checkpoints/'\n",
    "        self.global_step = tf.Variable(0, name = 'global_step', trainable = False)\n",
    "        \n",
    "        \n",
    "    def get_input_batches(self, images_batch_names, path):\n",
    "        images_arr = []\n",
    "        for image_name in images_batch_names:\n",
    "            \n",
    "            image = cv2.resize(cv2.imread(path + image_name), (256, 256))\n",
    "            img = cv2.normalize(image, None, 0, 128, cv2.NORM_MINMAX)\n",
    "            images_arr.append(img)\n",
    "        return np.asarray(images_arr)\n",
    "    \n",
    "    \n",
    "    def get_optimizers(self):\n",
    "        \n",
    "        with tf.variable_scope('optim', reuse = tf.AUTO_REUSE):\n",
    "            '''\n",
    "            Gen_XY transforms images from X to Y (e.g., Horse -> Zebra)\n",
    "            Gen_YX transforms images from Y to X (e.g., Zebra -> Horse)\n",
    "\n",
    "            Disc_X: scores how real an image of X looks (e.g. does this image look like a Horse?)\n",
    "            Disc_Y: scores how real an image of Y looks (e.g. does this image look like a Zebra?)\n",
    "\n",
    "            Generators generate fake images from originals\n",
    "            Discriminators discriminate whether the image is real or fake\n",
    "            '''\n",
    "            Gen_XY = Generator('Gen_XY', self.image_shape, self.is_training, 'instance', 'relu', self.batch_size, self.helper)\n",
    "            Gen_YX = Generator('Gen_YX', self.image_shape, self.is_training, 'instance', 'relu', self.batch_size, self.helper)\n",
    "\n",
    "            Disc_X = Discriminator('D_X', self.is_training, 'instance', 'leaky_relu', self.helper)\n",
    "            Disc_Y = Discriminator('D_Y', self.is_training, 'instance', 'leaky_relu', self.helper)\n",
    "\n",
    "\n",
    "            self.gen_xy = Gen_XY.g_feed_forward(self.X) # Generates fake_Y from X\n",
    "            self.gen_yx = Gen_YX.g_feed_forward(self.Y) # Generates fake_X from Y\n",
    "\n",
    "    #         disc_x = Disc_X.feed_forward(self.train_X)\n",
    "    #         disc_y = Disc_Y.feed_forward(self.train_Y)\n",
    "\n",
    "            gen_xyx = Gen_YX.g_feed_forward(self.gen_xy) # transforming fake_Y back to X\n",
    "            gen_yxy = Gen_XY.g_feed_forward(self.gen_yx) # transforming fake_X back to Y\n",
    "\n",
    "            disc_real_x = Disc_X.d_feed_forward(self.X) # probablilty of real X to be real\n",
    "            disc_real_y = Disc_Y.d_feed_forward(self.Y) # probability of reay Y to be real\n",
    "\n",
    "            disc_fake_x = Disc_X.d_feed_forward(self.gen_yx) # discriminating fake X (generated by Gen_YX)\n",
    "            disc_fake_y = Disc_Y.d_feed_forward(self.gen_xy) # discriminating fake Y (generated by Gen_XY)\n",
    "\n",
    "            '''\n",
    "            loss functions should comprise following:\n",
    "                - Discriminator should approve all the images from the dataset (train_X and train_Y)\n",
    "                - Discriminator should disapprove all the generated images (gen_xy and gen_yx)\n",
    "                - Generators should make the discriminators approve all the generated images\n",
    "                - The generated (fake) image should retain the properties of the original image i.e. if we generate a\n",
    "                  fake image using Gen_XY and transformed to original image using Gen_YX, the output of Gen_YX should possess\n",
    "                  the properties of the train_X image, thus satisfying cyclic-consistency. \n",
    "            '''\n",
    "\n",
    "            # Discriminator loss\n",
    "            loss_disc_real_x = tf.reduce_mean(tf.squared_difference(disc_real_x, 1))\n",
    "            loss_disc_real_y = tf.reduce_mean(tf.squared_difference(disc_real_y, 1))\n",
    "\n",
    "            loss_disc_fake_x = tf.reduce_mean(tf.square(disc_fake_x))\n",
    "            loss_disc_fake_y = tf.reduce_mean(tf.square(disc_fake_y))\n",
    "\n",
    "            self.disc_X_total_loss = (loss_disc_real_x + loss_disc_fake_x) / 2\n",
    "            self.disc_Y_total_loss = (loss_disc_real_y + loss_disc_fake_y) / 2\n",
    "\n",
    "            # Generator loss\n",
    "            '''\n",
    "            Generator should be successful in fooling the discriminator. In other words, Generator should make the discriminator\n",
    "            believe that the generated images are real images. This can be done if the recommendation made by the Discriminator\n",
    "            is as close to 1 as possible. so, Generator would like to minimize ((Discriminator_X(Generator_YX(image))) - 1)^2\n",
    "            '''\n",
    "            loss_gen_yx = tf.reduce_mean(tf.squared_difference(disc_fake_x, 1))\n",
    "            loss_gen_xy = tf.reduce_mean(tf.squared_difference(disc_fake_y, 1))\n",
    "\n",
    "            self.loss_cycle = tf.reduce_mean(tf.abs(self.X - gen_xyx) + tf.abs(self.Y - gen_yxy))\n",
    "\n",
    "            '''multiplied loss_cycle with 10 so as to give more importance to cycle loss than to discriminator loss'''\n",
    "            self.gen_xy_total_loss = loss_gen_xy + (10 * self.loss_cycle)\n",
    "            self.gen_yx_total_loss = loss_gen_yx + (10 * self.loss_cycle)\n",
    "\n",
    "            trainable_variables = tf.trainable_variables()\n",
    "            G_XY_vars = [var for var in trainable_variables if 'Gen_XY' in var.name]\n",
    "            G_YX_vars = [var for var in trainable_variables if 'Gen_YX' in var.name]\n",
    "            D_X_vars = [var for var in trainable_variables if 'D_X' in var.name]\n",
    "            D_Y_vars = [var for var in trainable_variables if 'D_Y' in var.name]\n",
    "            \n",
    "            \n",
    "            # optimizers\n",
    "            self.Gen_XY_optim = tf.train.AdamOptimizer(learning_rate = self.lr, beta1 = 0.5).minimize(self.gen_xy_total_loss, var_list = G_XY_vars)\n",
    "            self.Gen_YX_optim = tf.train.AdamOptimizer(learning_rate = self.lr, beta1 = 0.5).minimize(self.gen_yx_total_loss, var_list = G_YX_vars)\n",
    "            self.Disc_X_optim = tf.train.AdamOptimizer(learning_rate = self.lr, beta1 = 0.5).minimize(self.disc_X_total_loss, var_list = D_X_vars)\n",
    "            self.Disc_Y_optim = tf.train.AdamOptimizer(learning_rate = self.lr, beta1 = 0.5).minimize(self.disc_Y_total_loss, var_list = D_Y_vars)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def train_cycleGAN(self):\n",
    "    \n",
    "        print('training the cycle Gan')\n",
    "        self.get_optimizers()\n",
    "        \n",
    "            \n",
    "        train_X_images_path = self.train_data_path +'/trainA/'\n",
    "        train_Y_images_path = self.train_data_path +'/trainB/'\n",
    "\n",
    "        train_X_images_names = os.listdir(train_X_images_path)\n",
    "        train_Y_images_names = os.listdir(train_Y_images_path)\n",
    "        num_of_train_X_images = len(train_X_images_names)\n",
    "        num_of_train_Y_images = len(train_Y_images_names)\n",
    "\n",
    "        num_of_train_images = min(num_of_train_X_images, num_of_train_Y_images)\n",
    "        num_of_images_per_batch = num_of_train_images // self.batch_size\n",
    "\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        for epoch in range(self.session.run(self.global_step), self.num_of_epochs):\n",
    "\n",
    "            if self.to_restore:\n",
    "                checkpoint_file = tf.train.latest_checkpoint(self.check_point_dir)\n",
    "                saver.restore(self.session, checkpoint_file)\n",
    "\n",
    "            if not os.path.exists(self.check_point_dir):\n",
    "                os.makedirs(self.check_point_dir)\n",
    "\n",
    "            random.shuffle(train_X_images_names)\n",
    "            random.shuffle(train_Y_images_names)\n",
    "            if epoch >= 100:\n",
    "                self.lr = self.lr - self.lr * (epoch-100)/100\n",
    "\n",
    "            for index in range(0, num_of_images_per_batch, self.batch_size):\n",
    "                \n",
    "                train_x = self.get_input_batches(train_X_images_names[index : index + self.batch_size], train_X_images_path)\n",
    "                train_y = self.get_input_batches(train_Y_images_names[index : index + self.batch_size], train_Y_images_path)\n",
    "\n",
    "                # training Gen_XY_optim to generate fake Y images from true X\n",
    "                _, fake_Y = self.session.run([self.Gen_XY_optim, self.gen_xy], feed_dict = {self.X : train_x,\n",
    "                                                                                     self.Y : train_y,\n",
    "                                                                                     self.is_training : True\n",
    "                                                                                     })\n",
    "                # training Gen_YX_optim to generate fake X images from true Y\n",
    "                _, fake_X = self.session.run([self.Gen_YX_optim, self.gen_yx], feed_dict = {self.X : train_x,\n",
    "                                                                                         self.Y : train_y,\n",
    "                                                                                         self.is_training : True})\n",
    "                _ = self.session.run([self.Disc_X_optim], feed_dict = {self.X : train_x,\n",
    "                                                                    self.Y : train_y,\n",
    "                                                                    self.is_training : True})\n",
    "\n",
    "                _ = self.session.run([self.Disc_Y_optim], feed_dict = {self.X : train_x,\n",
    "                                                                    self.Y : train_y,\n",
    "                                                                    self.is_training : True})\n",
    "\n",
    "\n",
    "                print('epoch ' + str(epoch))\n",
    "                self.session.run(tf.assign(self.global_step, epoch + 1))\n",
    "                saver.save(self.session,self.output_dir + '/cycleGAN', global_step = epoch)\n",
    "\n",
    "                \n",
    "    \n",
    "    def create_directory(self, directory):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "                \n",
    "    \n",
    "    def apply_transformtions_on_test_images(self):\n",
    "       \n",
    "        print('testing the test images')\n",
    "        \n",
    "        test_images_output_dir = self.output_dir + '/test/'\n",
    "        \n",
    "        fake_X_path = test_images_output_dir + 'fake_X/'\n",
    "        fake_Y_path = test_images_output_dir + 'fake_Y/'\n",
    "        test_X_path = test_images_output_dir + 'test_X/'\n",
    "        test_Y_path = test_images_output_dir + 'test_Y/'\n",
    "        \n",
    "        self.create_directory(test_images_output_dir)\n",
    "        self.create_directory(fake_X_path)\n",
    "        self.create_directory(fake_Y_path)\n",
    "        self.create_directory(test_X_path)\n",
    "        self.create_directory(test_Y_path)\n",
    "        \n",
    "        \n",
    "        \n",
    "        test_X_images_path = self.test_data_path +'/testA/'\n",
    "        test_Y_images_path = self.test_data_path + '/testB/'\n",
    "        \n",
    "        test_X_images_names = os.listdir(test_X_images_path)\n",
    "        test_Y_images_names = os.listdir(test_Y_images_path)\n",
    "        \n",
    "        num_of_test_images = min(100, len(test_X_images_names), len(test_Y_images_names))\n",
    "        with tf.Session() as session:\n",
    "            meta_graph = tf.train.import_meta_graph(self.output_dir + '/cycleGAN-9.meta')\n",
    "            meta_graph.restore(session, tf.train.latest_checkpoint('./outputs/'))\n",
    "            \n",
    "            random.shuffle(test_X_images_names)\n",
    "            random.shuffle(test_Y_images_names)\n",
    "            for index in range(0, num_of_test_images):\n",
    "                \n",
    "                \n",
    "                test_x = self.get_input_batches(test_X_images_names[index : index + self.batch_size], test_X_images_path)\n",
    "                test_y = self.get_input_batches(test_Y_images_names[index : index + self.batch_size], test_Y_images_path)\n",
    "\n",
    "                #                 print('test_X ' + np.shape(test_x))\n",
    "                #                 print('test_Y ' + np.shape(test_Y))\n",
    "                fake_X, fake_Y = session.run([self.gen_yx, self.gen_xy], feed_dict = {self.X : test_x,\n",
    "                                                                                    self.Y : test_y,\n",
    "                                                                                    self.is_training : False})\n",
    "\n",
    "                print('fake X ' + str(np.shape(fake_X)))\n",
    "                print('fake Y ' + str(np.shape(fake_Y)))\n",
    "                cv2.imwrite((fake_X_path + str(index) + '_fx.png'), np.uint8(fake_X[0]))\n",
    "                cv2.imwrite((fake_Y_path + str(index) + '_fy.png'), np.uint8(fake_Y[0]))\n",
    "                cv2.imwrite((test_X_path + str(index) + '_tx.png'), np.uint8(test_x[0]))\n",
    "                cv2.imwrite((test_Y_path + str(index) + '_ty.png'), np.uint8(test_y[0]))\n",
    "                #                   count = count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LESZw071Ubi0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "988SLQDXUbi6"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    image_shape = [256, 256, 3]\n",
    "    batch_size  = 1\n",
    "    learning_rate = 0.0002\n",
    "    train_data_path = '/content/drive/My Drive/horse2zebra'\n",
    "    test_data_path = '/content/drive/My Drive/horse2zebra'\n",
    "    num_epochs = 10\n",
    "    to_restore = False\n",
    "    hf = helper_functions()\n",
    "    output_dir = './outputs'\n",
    "    \n",
    "    cycle_GAN = cycleGAN(image_shape, batch_size, learning_rate, train_data_path, test_data_path, num_epochs, \n",
    "                        to_restore, hf, output_dir)\n",
    "    cycle_GAN.train_cycleGAN()\n",
    "    cycle_GAN.apply_transformtions_on_test_images()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CycleGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
